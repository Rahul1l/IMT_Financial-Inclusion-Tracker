import streamlit as st
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.decomposition import NMF, PCA
from sklearn.feature_extraction.text import TfidfVectorizer

from textblob import TextBlob
import plotly.express as px
import pydeck as pdk


st.set_page_config(
    page_title="UPI Adoption Financial Tracker",
    layout="wide",
)


# -------- LOAD MULTI-SHEET EXCEL -------- #
@st.cache_data
def load_excel_sheets(file):
    xls = pd.ExcelFile(file)
    sheets = {s: xls.parse(s) for s in xls.sheet_names}
    return sheets


# -------- MERGE ALL 6 FACTOR SHEETS ON COMMON KEY -------- #
def merge_all_sheets(sheets):
    names = list(sheets.keys())
    df = sheets[names[0]].copy()

    # Detect common join key
    common_keys = set(df.columns)
    for s in names[1:]:
        common_keys &= set(sheets[s].columns)

    key = None
    if common_keys:
        # Prefer District or district-like column
        for c in common_keys:
            if "district" in c.lower():
                key = c
                break
        key = key or list(common_keys)[0]

    # Merge remaining sheets
    for i, s in enumerate(names[1:], start=2):
        df = df.merge(sheets[s], on=key, how="outer", suffixes=("", f"_{i}"))

    return df, key


# -------- BUILD ARTIFICIAL ADOPTION SCORE USING ALL NUMERIC FACTORS -------- #
def compute_upi_adoption_score(df):
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    if not numeric_cols:
        st.warning("No numeric columns found to build adoption score!")
        df["UPI_Adoption_Score"] = 50
        return df, numeric_cols

    X = StandardScaler().fit_transform(df[numeric_cols].fillna(df[numeric_cols].median()))
    pca = PCA(1, random_state=42)
    comp = pca.fit_transform(X).ravel()

    if comp.max() == comp.min():
        score = np.full_like(comp, 50.0)
    else:
        score = (comp - comp.min()) / (comp.max() - comp.min()) * 100

    df["UPI_Adoption_Score"] = score
    return df, numeric_cols


# -------------------------------------------------- #
# ---------------------- PAGES ---------------------- #
# -------------------------------------------------- #

def page_overview(df, key):
    st.subheader("Dataset Overview")
    if key:
        st.write(f"âœ… Joined using common key: `{key}`")
    st.write(f"Total rows: {df.shape[0]:,}  |  Total columns: {df.shape[1]:,}")
    st.dataframe(df.head())


def page_ml_model(df):
    st.subheader("ML Model â€“ Predict UPI Adoption Score Across Districts")

    target = "UPI_Adoption_Score"
    if "UPI_Adoption_Score" not in df.columns:
        st.error("âš  Adoption score column not found in dataset!")
        return

    features = [c for c in df.columns if c != target]
    X = df[features]
    y = df["UPI_Adoption_Score"]

    cat_cols = X.select_dtypes(include="object").columns.tolist()
    num_cols = X.select_dtypes(include=np.number).columns.tolist()

    pre = ColumnTransformer([
        ("num", Pipeline([("i", SimpleImputer("median")), ("s", StandardScaler())]), num_cols),
        ("cat", Pipeline([("i", SimpleImputer("most_frequent")), ("o", OneHotEncoder(handle_unknown="ignore"))]), cat_cols)
    ])

    model = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)
    pipe = Pipeline([("p", pre), ("m", model)])

    split = st.sidebar.slider("Test split", 10, 40, 20) / 100
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=split, random_state=42)

    if st.button("Train Model"):
        pipe.fit(X_tr, y_tr)
        pred = pipe.predict(X_te)
        r2 = r2_score(y_te, pred)
        mae = mean_absolute_error(y_te, pred)
        rmse = mean_squared_error(y_te, pred) ** 0.5

        st.metric("RÂ² Score", f"{r2:.4f}")
        st.metric("MAE", f"{mae:.4f}")
        st.metric("RMSE", f"{rmse:.4f}")

        if r2 >= 0.97:
            st.success("ðŸŽ¯ **RÂ² Target Achieved (â‰¥97%)** âœ…")
        else:
            st.warning("Try improving data, cleaning, or selecting important features.")

        chart = pd.DataFrame({"Actual": y_te, "Predicted": pred})
        fig = px.scatter(chart, x="Actual", y="Predicted", trendline="ols", title="Actual vs Predicted UPI Adoption")
        st.plotly_chart(fig, use_container_width=True)


def page_time_series(df):
    st.subheader("Time Series Forecast â€“ Digital Transaction Volume")

    # Detect date columns
    year = [c for c in df.columns if "year" in c.lower()]
    month = [c for c in df.columns if "month" in c.lower()]
    date = [c for c in df.columns if "date" in c.lower()]
    nums = df.select_dtypes(include=np.number).columns.tolist()

    if not nums:
        st.error("No numeric column found for forecasting!")
        return

    vol_col = st.selectbox("Select transaction volume column", nums)

    ts = None
    date_col = None

    if date:
        date_col = st.selectbox("Select date column", date)
        ts = df[[date_col, vol_col]].copy()
        ts[date_col] = pd.to_datetime(ts[date_col], errors="coerce")

    elif year and month:
        y = st.selectbox("Select Year column", year)
        m = st.selectbox("Select Month column", month)
        ts = df[[y, m, vol_col]].copy()
        ts[y] = pd.to_numeric(ts[y], errors="coerce")
        ts[m] = pd.to_numeric(ts[m], errors="coerce")
        ts = ts.dropna(subset=[y, m, vol_col])
        ts["combined_date"] = pd.to_datetime(
            ts[y].astype(int).astype(str) + "-" + ts[m].astype(int).astype(str) + "-01",
            errors="coerce"
        )
        date_col = "combined_date"

    else:
        st.error("No valid date structure found!")
        return

    ts = ts.dropna(subset=[date_col, vol_col]).sort_values(date_col)
    ts["t"] = np.arange(len(ts))
    model = RandomForestRegressor(350, random_state=42, n_jobs=-1)
    model.fit(ts[["t"]], ts[vol_col].values)

    steps = st.slider("Forecast months", 3, 24, 12)
    freq = "M"
    last = ts[date_col].iloc[-1]
    future_dates = pd.date_range(start=last, periods=steps+1, freq=freq)[1:]
    future_t = np.arange(ts["t"].iloc[-1]+1, ts["t"].iloc[-1]+1+steps)
    future_preds = model.predict(future_t.reshape(-1,1))

    fut_df = pd.DataFrame({date_col: future_dates, vol_col: future_preds, "type": "Forecast"})
    hist = ts[[date_col, vol_col]].copy()
    hist["type"] = "Actual"

    comb = pd.concat([hist, fut_df], ignore_index=True)
    fig = px.line(comb, x=date_col, y=vol_col, color="type", title="Actual vs Forecast - Digital Transaction Volume")
    st.plotly_chart(fig, use_container_width=True)
    st.dataframe(fut_df.head())


def page_text_analytics(df):
    st.subheader("Text Analytics & Sentiment (Factor Analysis Reports)")

    texts = df.select_dtypes(include="object").columns.tolist()
    if not texts:
        st.warning("No text columns for analysis!")
        return

    col = st.selectbox("Pick text column", texts)
    data = df[col].dropna().astype(str)

    k = st.slider("Topics", 2, 6, 3)
    vec = TfidfVectorizer(max_features=2000, stop_words="english")
    X = vec.fit_transform(data)
    nmf = NMF(n_components=k, random_state=42, init="nndsvda").fit(X)
    words = vec.get_feature_names_out()

    for i, t in enumerate(nmf.components_):
        topw = [words[idx] for idx in t.argsort()[-10:][::-1]]
        st.write(f"**Topic {i+1}:** " + ", ".join(topw))

    if st.checkbox("Sentiment analysis"):
        score = data.apply(lambda x: TextBlob(x).sentiment.polarity)
        fig = px.histogram(score, nbins=30, title="Sentiment Score Distribution")
        st.plotly_chart(fig, use_container_width=True)
        st.dataframe(score.head())


def page_geo_dashboard(df):
    st.subheader("Geo Dashboard â€“ UPI Adoption Score")

    lat = [c for c in df.columns if "lat" in c.lower()]
    lon = [c for c in df.columns if "lon" in c.lower() or "lng" in c.lower()]
    key_cols = df.columns.tolist()

    if not lat or not lon:
        st.error("âš  No Latitude / Longitude columns found!")
        return

    key = st.selectbox("Select district/label column", key_cols)
    lat_col = st.selectbox("Latitude column", lat)
    lon_col = st.selectbox("Longitude column", lon)

    geo = df[[key, lat_col, lon_col, "UPI_Adoption_Score"]].copy()
    for c in [lat_col, lon_col, "UPI_Adoption_Score"]:
        geo[c] = pd.to_numeric(geo[c], errors="coerce")
    geo = geo.dropna()

    if geo.empty:
        st.error("No valid geo rows left!")
        return

    r = geo["UPI_Adoption_Score"]
    geo["r"] = 3000 + (r - r.min())/(r.max()-r.min()+1e-5)*9000

    layer = pdk.Layer(
        "ScatterplotLayer",
        geo,
        get_position=[lon_col, lat_col],
        get_radius="r",
        pickable=True,
    )

    st.pydeck_chart(pdk.Deck(layer, initial_view_state=pdk.ViewState(
        latitude=geo[lat_col].mean(),
        longitude=geo[lon_col].mean(),
        zoom=4
    )))

    st.dataframe(geo.head())


# -------------------------------------------------- #
# ---------------------- MAIN ----------------------- #
# -------------------------------------------------- #

def main():
    file = st.sidebar.file_uploader("Upload capstone workbook", ["xlsx","csv"])

    if file:
        sheets = load_excel_sheets(file)
        df, key = merge_all_sheets(sheets)
        df, _ = compute_upi_adoption_score(df)
        st.session_state["df"] = df
        st.session_state["key"] = key

    if "df" not in st.session_state:
        st.title("Upload your capstone dataset to begin")
        return

    df = st.session_state["df"]

    nav = st.sidebar.radio("Navigation", [
        "Overview",
        "ML Model",
        "Time Series Forecast",
        "Text Analytics",
        "Geo Dashboard"
    ])

    if nav=="Overview":
        page_overview(df, st.session_state["key"])
    if nav=="ML Model":
        page_ml_model(df)
    if nav=="Time Series Forecast":
        page_time_series(df)
    if nav=="Text Analytics":
        page_text_analytics(df)
    if nav=="Geo Dashboard":
        page_geo_dashboard(df)


if __name__ == "__main__":
    main()
